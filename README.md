# Scale leads to compositional generalization (Vision)

This project investigates compositional generalization in generative vision models, particularly diffusion models. It provides tools to:
1.  Generate images based on compositional prompts.
2.  Extract internal model activations during the generation process.
3.  Train decoder models on these activations to predict the latent concepts present in the prompts.
4.  Evaluate the generalization capacity of models to out-of-distribution (OOD) compositional prompts using these decoders.
5.  Perform LLM-based evaluation (e.g., using Gemini) of generated images against compositional schemas.
6.  Rank OOD samples based on various metrics.

## ðŸš§ Installation

We use `uv` for Python dependency management. To install all necessary dependencies, run the following command in your terminal from the project root:

```bash
uv sync 
```


## ðŸš€ Running Analyses

This project offers several scripts to conduct different types of analyses. All commands should be run from the project's root directory.

### A. Decoder-Based Compositional Analysis (using `run_experiment.py`)

This is the primary script for running end-to-end experiments involving image generation, activation extraction, decoder training, and OOD evaluation.

**Purpose:**
* Generates a dataset of images based on a specified compositional setup.
* Extracts activations from various layers of the diffusion model during image generation.
* Trains linear decoders (Logistic Regression) to predict the ground truth latent components from these activations.
* Evaluates the decoders' performance on both in-distribution (train) and out-of-distribution (test) samples.
* Ranks OOD samples based on decoder performance and selected metrics.

**Command Structure:**
```bash
uv run python run_experiment.py \
    --model-name "stabilityai/stable-diffusion-xl-base-1.0" \
    --comp-setup "animals_with_clothes" \
    --output-dir "./experiment_results/sdxl_animals_clothes" \
    --num-seeds 10 \
    --reuse-existing
```

### B. LLM-Based Generalization Evaluation (using `llm_eval.py`)

This script evaluates previously generated images using a Large Language Model (LLM) like Gemini. It assesses whether the LLM can correctly identify the components in the images based on a dynamically generated schema.

**Purpose:**
* Loads images generated by `run_experiment.py`.
* Uses an LLM to analyze each image against a schema derived from the compositional setup.
* Compares LLM predictions to ground truth latents.
* Reports accuracy and saves detailed evaluation results.

**Command Structure:**
```bash
uv run python llm_eval.py \
    --model-name "stabilityai/stable-diffusion-xl-base-1.0" \
    --comp-setup "animals_with_clothes" \
    --base-dir "./experiment_results/sdxl_animals_clothes" \
    --gemini-model "gemini-1.5-flash-latest" \
    --max-images 1000
```
